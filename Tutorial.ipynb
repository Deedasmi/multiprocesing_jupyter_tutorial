{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiprocessing tutorial\n",
    "By Richard Petrie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The goal of this tutorial\n",
    "In this tutorial we will cover why we would want to use multi-processing to do data analytics. In class we've only worked on small data sets. A few hundred lines, maybe a couple megabytes. In this tutorial we will analyze a 4.3GB file containing more than 446 million passwords. We will face two major challenges\n",
    "\n",
    "First, with large files we won't be able to fit the entire dataset into memory. While we technically could with this 4.3GB file, I hope to do similiar work on a 170GB file in the future. That file definitly would not fit in our machine's memory. We beat this by only reading a part of the file at a time.\n",
    "\n",
    "Second, there is a lot of work to do. Every operation I want to perform on these words, like finding the most common letter, I have to do 446 million times. Since each password can be analyzed individually regardless of how the other passwords look, we can do multiple words at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is multi processing?\n",
    "In short, it's using multiple processes to do a single job. Here is a handy picture that does a good job representing what is happening.\n",
    "\n",
    "![apprize](multiprocess.jpg)\n",
    "\n",
    "As you can see, the child proceses are talking back to the main process who is the only one reading from the disk. This means we don't have multiple processes reading form the file (which is VERY hard), but we can put the child processes to work.\n",
    "\n",
    "Without further ado, on to the code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The imports\n",
    "We used Counter and csv in class, so I'm going to skip those two.\n",
    "\n",
    "* time: We use this module to track how long our code takes to run, so we know how much faster it is to run concurrently.\n",
    "* os: I developed this on both a Windows and Linux machine, this library makes it easier for me to switch between my machines.\n",
    "* multiprocessing: This is the core 'magic' of the program. Python's built in module for opening child processes and communicating with them. The \"Pool\" object here is an easy way to create a 'resource pool'. The \"Queue\" object is used so the main process can communicate with the child processes\n",
    "* multiprocessing.dummy: There is a bug on Windows that prevents the main multiprocessing library from working inside Jupyter. This library fixes that.\n",
    "* queue: I import an error code here so I can know when we are done communicating with the child processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import csv\n",
    "from collections import Counter\n",
    "import os\n",
    "if os.name == \"nt\":\n",
    "    from multiprocessing.dummy import Pool, Queue\n",
    "else:\n",
    "    from multiprocessing import Pool, Queue\n",
    "from queue import Empty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants\n",
    "We haven't done this in class, but I use some values multiple times throughout my entire program, and they never change. To make it easier, I keep all of those values up here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will read up to 50,000 passwords from D:/password/no_null.csv\n"
     ]
    }
   ],
   "source": [
    "# Constants.\n",
    "# Change this to change program function\n",
    "\n",
    "# The file with the passwords\n",
    "if os.name == \"nt\":\n",
    "    FILEPATH = \"D:/password/no_null.csv\"\n",
    "else:\n",
    "    FILEPATH = \"/home/richardp/passwords/no_null.csv\"\n",
    "\n",
    "# Number of passwords to read from disk before passing to CPU\n",
    "PER_ITER = 1000#000\n",
    "\n",
    "# Number of iterations to to run\n",
    "ITER_MAX = 50#0\n",
    "total_pass = PER_ITER * ITER_MAX\n",
    "print(f\"We will read up to {total_pass:,} passwords from {FILEPATH}\")\n",
    "\n",
    "# Process inter-communication\n",
    "jobs = Queue(maxsize=6)\n",
    "results = Queue()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The first problem: Dealing with large datasets\n",
    "Having a lot of data is both a good thing. It allows us to find more interesting facts about our data. For example, I could ask this dataset if the most common third character in a password was a number or letter. With only 100 passwords I don't have significant confidence in what my data is telling me. With nearly 450 million passwords I can make a pretty effective guess. \n",
    "\n",
    "However, it is also a burdan. It takes time to process all that data. For example, my test case has been a 4.3GB csv file that contains no columns, only a single password per line. When we attempt to load this into a pandas dataframe, pandas attempts to analyze it all at once and does so very ineffeciently. When I attempted this on my computer with 16 GB of RAM, pandas kept trying to analyze my 4.3GB file until it had taken up nearly 25 GB of memory, well more than my computer actually has. It didn't even finish, I had to kill the process.\n",
    "\n",
    "![](pandas.PNG)\n",
    "\n",
    "To solve this problem, we write something like this. This code is a special type of function called a generator. It allows us to read small pieces of the file, and analyze chunks of it at a time instead of all at once. This does prevent us from doing any type of correlation analysis directly on the data, as we will never have all of the data at once. In my case, this is fine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_passwords(per_iter=PER_ITER, iterations=ITER_MAX):\n",
    "    with open(FILEPATH, newline='', encoding=\"latin1\") as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter='\\n', quoting= csv.QUOTE_NONE)\n",
    "        # This loop controls how many times you read from the file.\n",
    "        # This is useful if you don't want to read the entire file.\n",
    "        for _ in range(iterations):\n",
    "            words = []\n",
    "            # This loop controls how many passwords you read each time you loop over it.\n",
    "            # Useful for controlling the amount of RAM in use.\n",
    "            for _ in range(per_iter):\n",
    "                try:\n",
    "                    word = next(reader)\n",
    "                    words.append(word)\n",
    "                except StopIteration:\n",
    "                    break\n",
    "            yield words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Not using pandas\n",
    "The problem remains that Pandas is doing significantly more calculations than we need for our data. This means that in order to analyze our data we will be dealing with a lot of things we don't need, which we will call overhead. In this case, at least 20GB of data that was loaded we didn't need. This severely increases the time it takes to analyze large data sets. So we have decided to not use Pandas, and instead build our own analysis tools. The Data class below acts very similiar to a pandas DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Data:\n",
    "    # Similiar to how you might call pd.DataFrame() to create a DataFrame\n",
    "    # you could call Data() to create this Data object.\n",
    "    # __init__ is a special function that helps create the object\n",
    "    def __init__(self, words=None):\n",
    "        # This field will track the length of every password\n",
    "        self.len_counter = Counter()\n",
    "        # This value will equal the number of passwords we analyzed\n",
    "        self.word_count = 0\n",
    "        # This is useful for troubleshooting how many Data objects we created\n",
    "        self.iterations = 1\n",
    "        # Finally, this counts how often every character is used in all the passwords we read\n",
    "        # This is a very CPU intensive task. We'll explain more in the calculate function\n",
    "        self.unique_chars = Counter()\n",
    "        \n",
    "        if words:\n",
    "            self.calculate(words)\n",
    "    \n",
    "    # This function is called when you attempt to add Data to something else\n",
    "    # In this case, it errors if that something else isn't also a Data object\n",
    "    def __add__(self, other):\n",
    "        if type(other) != Data:\n",
    "            raise TypeError(f\"Added {type(other)} to Data\")\n",
    "            \n",
    "        # This is a small optimization to make things faster\n",
    "        if self.word_count == 0:\n",
    "            return other\n",
    "        if other.word_count == 0:\n",
    "            return self\n",
    "        \n",
    "        # This code actually does all of the addition\n",
    "        self.word_count += other.word_count\n",
    "        self.len_counter.update(other.len_counter)\n",
    "        self.iterations += other.iterations\n",
    "        self.unique_chars.update(other.unique_chars)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    # This is a special function that is called when you print() something\n",
    "    def __repr__(self):\n",
    "        return f\"We found {self.word_count:,} passwords.\"\n",
    "    \n",
    "    # We use this function to populate our values with the data we read from the CSV\n",
    "    def calculate(self, words):\n",
    "        for word in words:\n",
    "            w = str(word)\n",
    "            self.len_counter[len(w)] += 1\n",
    "            self.word_count += 1\n",
    "            \n",
    "            # This one line of code takes more time to run\n",
    "            # then all the other code here combined.\n",
    "            # It counts ever letter in a word, for all hundreds of millions of words\n",
    "            self.unique_chars.update(w)\n",
    "    \n",
    "    def get_num_chars(self):\n",
    "        return sum(data.unique_chars.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Fun Part: Multiprocessing\n",
    "Multiprocessing is hard to explain. We are actually starting new python child processes to do our work for us. We are telling them to line up in front of a job board, and posting password lists to the board. They take the password list, do the calculations, and put the results in a different board. Then they get back in line for a new job. At the end, we collect all the results and combine them together.\n",
    "\n",
    "To keep our test code below clean, we've also made an object just to schedule and run our jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class JobHandler:\n",
    "    # You start a job handler with the number of processes you want to spawn\n",
    "    # so JobHandler(2) would start 2 child processes\n",
    "    def __init__(self, cores):\n",
    "        self.cores = cores\n",
    "        # This code spawns a number of child processes and tells them to run the counter \n",
    "        # function when they start\n",
    "        self.pool = Pool(cores, initializer=self.counter, initargs=(jobs, results))\n",
    "        self.word_generator = word_generator\n",
    "    \n",
    "    # This is the main function that you would run to do all the work.\n",
    "    def run(self,  word_generator):\n",
    "        self._schedule(word_generator)\n",
    "        return self._get_results()\n",
    "    \n",
    "    # We start a function with an underscore to indicate that you should not run it directly\n",
    "    # These function are for the object to use, not for the user to call.\n",
    "    # In this case, this function reads the data from the csvfile\n",
    "    # and tells the child processes what to do. Afterwards, it cleans them all up\n",
    "    def _schedule(self, word_generator):\n",
    "        for df in word_generator:\n",
    "            jobs.put(df)\n",
    "        for _ in range(self.cores): #tell workers we're done\n",
    "            jobs.put(None)\n",
    "        self.pool.close()\n",
    "        self.pool.join()\n",
    "\n",
    "    def _get_results(self):\n",
    "        data = Data()\n",
    "        while True:\n",
    "            try:\n",
    "                data += results.get_nowait()\n",
    "            except Empty:\n",
    "                break\n",
    "        return data\n",
    "    \n",
    "    # This is the function that the child processes will run in\n",
    "    # It does all the actual work to analyze the data\n",
    "    @staticmethod\n",
    "    def counter(jobs, response):\n",
    "        data = Data()\n",
    "        while True:\n",
    "            my_job = jobs.get()\n",
    "            if my_job is not None:\n",
    "                data += Data(my_job)\n",
    "            else:\n",
    "                response.put(data)\n",
    "                break\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So wait, why did we do this again?\n",
    "\n",
    "This code below opens our generator, and runs through the entire passowrd file in a single process. This is the 'normal' version, with no special tricks. We still only read small chunks of the file at a time, but the same process reads from the file and does the calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done after 50 iterations. Took 0.24558401324291343 seconds\n",
      "We found 50,000 passwords.\n",
      "We counted 451,076 characters in total!\n"
     ]
    }
   ],
   "source": [
    "# Not threaded\n",
    "word_generator = read_passwords()\n",
    "data = Data()\n",
    "start = time.perf_counter()\n",
    "for words in word_generator:\n",
    "    data += Data(words)\n",
    "    \n",
    "elapsed = time.perf_counter() - start\n",
    "print(f\"Done after {data.iterations} iterations. Took {elapsed} seconds\")\n",
    "print(data)\n",
    "print(f\"We counted {data.get_num_chars():,} characters in total!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alright, so what's the big deal?\n",
    "\n",
    "This code is the concurent one, which can both read from the file, and do the analysis at the same time. In fact, it does 4 sections of the CSV file all at the same time while the main process is the one reading from the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done after 50 iterations. Took 0.3155867838860333 seconds\n",
      "Difference was -24.948828770587117%\n",
      "We found 50,000 passwords.\n",
      "We counted 451,076 characters in total!\n"
     ]
    }
   ],
   "source": [
    "# Threaded\n",
    "start = time.perf_counter()\n",
    "data = JobHandler(4).run(read_passwords())\n",
    "elapsed2 = time.perf_counter() - start\n",
    "print(f\"Done after {data.iterations} iterations. Took {elapsed2} seconds\")\n",
    "diff = (elapsed - elapsed2) / ((elapsed + elapsed2) / 2) * 100\n",
    "print(f\"Difference was {diff}%\")\n",
    "print(data)\n",
    "print(f\"We counted {data.get_num_chars():,} characters in total!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Oh hey, that was fast\n",
    "Yep! Thanks to the fact that we can process multiple pieces of our data set at once, we see a speedup of around 50%. And you can see that it found the same number of passwords, and the same number of characters! This way we know the the process works, and it allows us to do our work so much faster! If this scales linerally, on my 170GB file I will save hours on every run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Sources\n",
    "https://apprize.info/python/hdf5/9.html\n",
    "\n",
    "https://lethain.com/handling-very-large-csv-and-xml-files-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
